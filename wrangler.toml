name = "tolldns-seo-agent"
main = "src/worker.ts"
compatibility_date = "2026-02-24"

# If you use Node APIs in tooling, keep this:
# compatibility_flags = ["nodejs_compat"]

[triggers]
crons = ["0 9 * * *"]

# --- Bindings ---
# D1 (fill database_id after creation)
[[d1_databases]]
binding = "DB"
database_name = "SEO_AI_Agent"
database_id = "e5b22404-32e9-4a5c-a890-1effb9266c9c"

# Workers AI (embeddings etc. later; safe to keep now)
[ai]
binding = "AI"

# Vectorize (optional for your semantic memories; create later if you want)
# Comment these out if youâ€™re not using Vectorize yet.
[[vectorize]]
binding = "USER_MEM"
index_name = "user-memory"

[[vectorize]]
binding = "GLOBAL_TRENDS"
index_name = "global-trends"

[[vectorize]]
binding = "DESIGN_CATALOG"
index_name = "design-catalog"

[[r2_buckets]]
binding = "MEMORY_R2"
bucket_name = "seo-ai-memory"

# --- Environment variables (non-secret) ---
[vars]
# Optional: default PSI strategy if site row doesn't specify
DEFAULT_PSI_STRATEGY = "mobile"
# Optional: Apify actor for Google SERP (headless Chrome)
APIFY_GOOGLE_ACTOR = "apify/google-search-scraper"

# --- Secrets (set via wrangler secret put) ---
# PAGESPEED_API_KEY is required for PSI calls:
#   npx wrangler secret put PAGESPEED_API_KEY
# APIFY_TOKEN is required for /serp/google/top20 and /serp/sample scraping:
#   npx wrangler secret put APIFY_TOKEN
# PROXY_CONTROL_SECRET protects /proxy/admin/* routes:
#   npx wrangler secret put PROXY_CONTROL_SECRET
